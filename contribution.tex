\chapter*{Individual contribution}
In this section, we list the contribution of each of us to this project. We must make some caveats about the content of this section. First, some listed work did not make it into this memoir, but was nevertheless instrumental to our final discoveries, so it has been added here. 

Second, a large part of our work was fully cooperative. Most notably, the organization of the work itself was carried out together: we identified and redacted the main objectives, set preliminary deadlines and set up routine meetings, where we would share our progress and ideas.

We wrote the memoir asynchronously and iteratively, without a strict division of sections.  We used a similar process to develop code, routinely using pair programming in order to double-check the code and ascertain our familiarity with the code base. Consequently, the individual contributions listed here are only indicative. 

Last, we note that \Cref{chapter6} is an adapted version of an article to the International Conference on Case-Based Reasoning (ICCBR) prepared in cooperation with the director of this work, Professor Antonio Alejandro Sánchez Ruiz-Granados, who set up, carried out and redacted the experiments present in \Cref{sec:experiments} using data we had previously extracted. 

\subsection*{Luis Ariza López}
Since we both knew almost nothing about \textit{Deep Learning}, we started by trying to find resources to educate ourselves on the topic. The first reference found was the \textit{Deep Learning Specialization} \cite{andrewNg} course that gave us a good perspective on this area. Once I became more familiar with these new concepts, I created a curated list of books and courses, from academic textbooks (like \textit{Deep Learning} \cite{goodfellow2016deep}) to practical tutorials, which served both as an introduction to the topic and as a reference. I also redacted a distilled explanation of the fundamentals of neuronal networks, which would serve as a draft of \Cref{chapter2}. 

With all these new concepts learned, I prepared a minimal project based on a previous tutorial \cite{whereWaldo} that would serve us to familiarize ourselves with convolutional neural network. In the project, we defined from scratch a convolutional neural network using \textit{Pytorch} and trained it to play \textit{Where's Wally?}, trying to identify the character in a set of dynamically created images. The project, which we completed successful together, was also useful as a first contact with \textit{Albumentations} and \textit{OpenCV}, two libraries for image manipulation we would use extensively during the project.

In order to sketch out a good approach for our problem, DR grading, I carried out a literature search. I tried to identify papers which tried to address the problem of DR grading from fundus images and studied them. The analysis paid attention to multiple characteristics: the design ideas, the architecture used, the results achieved and the explainability properties of the approaches. The writing down of my findings would become the skeleton for \Cref{sec:diagnosis}.

As part of this process I also created a draft explaining the approach of winning teams to the Kaggle competition, in order to identify solid strategies. While we did not take design ideas from the competition, as most solutions relied on extensively in ensembles, my search identified excellent techniques for preprocessing the dataset. Once we chose a dataset, I led the process of exploration and preprocessing, redacted in \Cref{chapter4}. This project included exploratory analysis to understand the dataset distribution and testing different preprocessing techniques, including histogram normalization and equalization (and some advanced techniques like CLAHE). I also implemented some of these techniques, as PCA augmentation. Once we decided to use Gaussian noise, I preprocessed the database, cropping and scaling the images and adding Gaussian noise.

In order to show as much as information possible and as wide a range of cases as possible, I carried out a detailed investigation over the dataset and the result of its preprocessing. The most significant images are presented in \Cref{chapter4}.

I also researched different techniques we could use to make sure the training was not too affected by the irregular distribution of classes, including custom loss functions, over sampling or under sampling. We tested all these methods, although we ended up not including them, since they severely degraded the performance of the model.

After the network was trained, I studied the problem of calibration. It took a bit of reading to understand why networks should be calibrated (it is a topic usually disregarded in literature) but I identified a reliable way to it: temperature scaling. I adapted the code from the team introducing this technique to our model and used it to calculate the \textit{temperature constant}, that would serve to calibrate our model. I also developed the code to create the ECE curve, that made certain the model was indeed well calibrated.

On the topic of interpretability I identified several model interpretation techniques that we could use, including CAM, Grad-CAM, anchor maps and saliency maps. I redacted a document discussing each of these techniques, including a curated list of techniques and references that could be used to implement them for our model.

I also developed most of the code necessary to perform the experiments mentioned in \Cref{sec:experiments}. From the architecture created by Álvaro to extract the embeddings, I defined a k-NN classifier and evaluated its performance for different metrics and values of $k$. I also created the sample we would use for the test: from a randomly selected set of images from the test dataset showing all the levels of the disease, I developed a program that selected images from the train dataset with different degrees of similarity, labelled them and stored them appropriately. These images were later shown to the clinician, who assessed its helpfulness as an assist to diagnosis.

% Faltan dos párrafos:
% - Algo de lo del ICCBR
% - Algo de redacción de la memoria

\subsection*{Álvaro Sanz Ramos}
Following our initial distribution, while Luis identified resources to learn about deep learning I researched about about diabetic retinopathy and its detection. I studied the cause of the disease and its signs and symptoms and tried to understand the diagnostic difficulties by attempting to apply medical guidelines to diagnose test images. The redacted findings can now be found in \Cref{sec:disease}.

Once I became familiar with deep learning (thanks to the initial project and my own readings on the topic) I explored the main approaches to computer vision, focusing on the difference between convolutional neural networks and transformers. We were initially unsure about which of the two architectures we should use, but after some research I became convinced convolutional neural networks was the wise choice. After explaining my considerations to Luis, we decided to use convolutional neural networks as the backbone of our model.

Initially, we worked in Luis' computer, which we had turn into a server. We would develop code using the \textit{JupyterHub} environment we had installed. Soon, it became evident this model led to poorly developed code, as Jupyter notebooks enforce questionable software engineering practices. In that time, Professor Sánchez gave us access to a server to train our model and I spent some time configuring the development environment and the necessary libraries. Instead of Jupyter notebooks, we would develop the code locally in Python modules, copy it to the server through SCP and execute it there, using an interpreter over SSH. We were quite satisfied with this workflow, automated using Jetbrains \textit{Pycharm} IDE, as it made developing fluent and led to well-structured programs.

Since we had decided to use convolutional neural networks as the backbone of our model, I spent some time identifying the most relevant CNN models we could use for our project. I found four potential families of candidates: ResNeXt, ConvNeXt, EfficientNetv2 and ResNet-RS. I trained different versions of these families on a reduced version of the preprocessed dataset and compared the results and the training efficiency. Since it was significantly faster to train than the others and achieved solid results, we agreed to use EfficientNetv2.

Once we had chosen to use an EfficientNet architecture I studied the EfficientNet and EfficientNetv2 original papers, in order to get a firm understanding of the network architecture. The redacted results are now in \Cref{sec:efficientnet}. 

As part of the previous process of evaluating multiple models, I developed the code to train the model. This was a delicate process, as I had to address several concerns: loading all the data, correctly doing forward and backward propagation, evaluating the progress of training on the validation dataset… The code was architecture-agnostic and was structured to make changing the model or the hyperparameters easy. 

Once we had settled on a definitive version, I trained our backbone model, an EfficientNetV2-B3 network. It took some work to identify a performant optimization method and choice of hyperparameters and I supervised training to spot instances of over and under fitting. Once the backbone was trained, I evaluated its performance. While the results were solid, the research Luis had done pointed that binocular methods could improve accuracy, which led me to the approach described in \Cref{sec:blending}. I also tried several other ideas to improve performance, including the enhanced way to create predictions from the logits described in the aforementioned section. I also developed the code to create a confusion matrix and the ROC curve, in order to evaluate the model.

Since we wanted to explore some lateral uses of the model, I tried to extract the embeddings of the network. As we have explained in the main text, these embeddings were too large to be effectively used and common techniques for dimensionality reduction severely degraded its quality. Professor Sánchez proposed to use the model itself to perform dimensionality reduction, which led me to the architecture shown in \Cref{fig:model_headed}. This architecture was later used to prepare the article for the ICCBR and to generate, using Luis code, the results shown in \Cref{chapter6}.

To improve the interpretability of the model, I implemented some techniques Luis had identified and extracted the feature maps and weights maps of the network. This process required a careful study of the original papers and introduced me to the use of \textit{hooks}, an abstraction from event-driven programming that can be used to model network introspection in \textit{Pytorch}. I found that the visualization generated by CAM and GradCAM was usually too noisy to be interpreted, so I devised a method to extract only the most significant parts of the mask and overlay it over the original image using pseudocolor. This work is described in \Cref{chapter7}.

Finally, in order to test my initial bet for CNNs, I identified a suitable vision transformer model we could use. The chosen ViT version had been pretrained for DR grading by a team of researchers and was able to overcome some limitations of the original ViT by using \textit{multiple instance learning}. I trained the vision transformer with different hyperparameters and evaluate its performance. The findings can be found in \Cref{chapter8}.